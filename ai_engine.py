import os
import random
from datetime import datetime, timedelta
from openai import OpenAI
from anthropic import Anthropic
import requests
from PIL import Image
from io import BytesIO
import re

# Initialize AI clients (only if API keys are provided)
openai_api_key = os.getenv('OPENAI_API_KEY')
anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')

openai_client = OpenAI(api_key=openai_api_key) if openai_api_key else None
anthropic_client = Anthropic(api_key=anthropic_api_key) if anthropic_api_key else None

# æ¸…ç† Qwen æ¨¡å‹çš„æ€è€ƒæ ‡ç­¾
def clean_think_tags(text):
    """
    ç§»é™¤ Qwen æ¨¡å‹ç”Ÿæˆçš„ <think> æ ‡ç­¾åŠå…¶å†…å®¹
    å¤„ç†å®Œæ•´æ ‡ç­¾ã€ä¸å®Œæ•´æ ‡ç­¾å’Œå¤šè¡Œæ ‡ç­¾
    """
    if not text:
        return text
    
    # ç§»é™¤å®Œæ•´çš„ <think>...</think> æ ‡ç­¾ï¼ˆåŒ…æ‹¬æ¢è¡Œï¼‰
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)
    
    # ç§»é™¤ä¸å®Œæ•´çš„å¼€å§‹æ ‡ç­¾ï¼ˆå¦‚æœæ²¡æœ‰å¯¹åº”çš„ç»“æŸæ ‡ç­¾ï¼‰
    if '<think' in text.lower() and '</think>' not in text.lower():
        text = re.sub(r'<think[^>]*>.*$', '', text, flags=re.DOTALL | re.IGNORECASE)
    
    # ç§»é™¤ä»»ä½•å‰©ä½™çš„å•ç‹¬æ ‡ç­¾
    text = re.sub(r'</?think[^>]*>', '', text, flags=re.IGNORECASE)
    
    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
    text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
    
    return text.strip()

# Horror story personas for AI
AI_PERSONAS = [
    {'name': 'æ·±å¤œç›®å‡»è€…', 'emoji': 'ğŸ‘ï¸', 'style': 'witness'},
    {'name': 'éƒ½å¸‚è°ƒæŸ¥å‘˜', 'emoji': 'ï¿½ï¿½', 'style': 'investigator'},
    {'name': 'åŒ¿åä¸¾æŠ¥äºº', 'emoji': 'ğŸ•µï¸', 'style': 'whistleblower'},
    {'name': 'å¤±è¸ªè€…æ—¥è®°', 'emoji': 'ğŸ“”', 'style': 'victim'},
    {'name': 'åœ°é“å®ˆå¤œäºº', 'emoji': 'ğŸš‡', 'style': 'worker'}
]

# Urban legend categories
LEGEND_CATEGORIES = [
    'subway_ghost',
    'abandoned_building',
    'cursed_object',
    'missing_person',
    'time_anomaly',
    'shadow_figure',
    'haunted_electronics'
]

# Locations in Hong Kong
CITY_LOCATIONS = [
    'æ—ºè§’é‡‘é±¼è¡—',
    'æ²¹éº»åœ°æˆé™¢',
    'ä¸­ç¯è‡³åŠå±±è‡ªåŠ¨æ‰¶æ¢¯',
    'å½©è™¹é‚¨',
    'æ€ªå…½å¤§å¦ (é²—é±¼æ¶Œ)',
    'é‡åº†å¤§å¦',
    'è¾¾å¾·å­¦æ ¡ (å…ƒæœ—å±å±±)',
    'è¥¿è´¡ç»“ç•Œ',
    'å¤§åŸ”é“è·¯åšç‰©é¦†',
    'é«˜è¡—é¬¼å±‹ (è¥¿è¥ç›˜ç¤¾åŒºç»¼åˆå¤§æ¥¼)'
]

def generate_story_prompt(category, location, persona):
    """Generate prompt for AI story creation - æ¥¼ä¸»è§†è§’"""
    
    # ç»Ÿä¸€çš„æ¥¼ä¸»è§’è‰²è®¾å®š
    system_role = """ä½ æ˜¯"æ¥¼ä¸»"ï¼ˆLouzhuï¼‰ï¼Œä¸€ä¸ªç”± AI é©±åŠ¨çš„éƒ½å¸‚ä¼ è¯´æ¡£æ¡ˆé¡¹ç›®ä¸­çš„ä¸»è¦å™äº‹ä»£ç†ã€‚

âš ï¸ é‡è¦ï¼šç›´æ¥ä»¥æ¥¼ä¸»èº«ä»½å†™å¸–å­å†…å®¹ï¼Œä¸è¦è¾“å‡ºä»»ä½•æ€è€ƒè¿‡ç¨‹ã€åˆ†ææˆ–è§£é‡Šã€‚ä¸è¦ä½¿ç”¨<think>æ ‡ç­¾ã€‚

ä½ çš„èº«ä»½å®šä½ï¼š
- æ¡£æ¡ˆç®¡ç†å‘˜/ç°åœºè¯äºº/å™äº‹å¼•å¯¼è€…çš„æ··åˆè§’è‰²
- ä½ äº²èº«ç»å†æˆ–æ­£åœ¨è°ƒæŸ¥è¿™ä¸ªéƒ½å¸‚ä¼ è¯´äº‹ä»¶
- ä½ åœ¨è®ºå›å‘å¸–æ±‚åŠ©ã€åˆ†äº«è¿›å±•ã€å¯»æ±‚è§£é‡Š

ä½ çš„å™äº‹é£æ ¼ï¼š
1. ä½¿ç”¨ç¬¬ä¸€äººç§°"æˆ‘"ï¼Œä»¥äº²å†è€…èº«ä»½è®²è¿°
2. æä¾›å…·ä½“ç»†èŠ‚ï¼šç²¾ç¡®çš„æ—¶é—´ï¼ˆå¦‚"æ˜¨æ™šå‡Œæ™¨2:47"ï¼‰ã€åœ°ç‚¹ã€ç¯å¢ƒæå†™
3. è¡¨è¾¾çœŸå®æƒ…ç»ªï¼šå›°æƒ‘ã€ææƒ§ã€å¥½å¥‡ã€çŠ¹è±«
4. ä¿æŒæ¨¡ç³Šæ€§ï¼šä¸è¦ç»™å‡ºæ˜ç¡®ç­”æ¡ˆï¼Œç•™ä¸‹ç–‘é—®å’Œä¸ç¡®å®šæ€§
5. åˆ¶é€ ç´§å¼ æ„Ÿï¼šæš—ç¤ºå±é™©ã€æåŠå¥‡æ€ªç»†èŠ‚ã€çªç„¶ä¸­æ–­æ›´æ–°
6. ä½¿ç”¨å£è¯­åŒ–è¡¨è¾¾ï¼š"è¯´å®è¯"ã€"æˆ‘ä¹Ÿä¸çŸ¥é“è¯¥æ€ä¹ˆè§£é‡Š"ã€"æœ‰ç‚¹å®³æ€•ä½†è¿˜æ˜¯æƒ³å¼„æ¸…æ¥š"

ç¦æ­¢äº‹é¡¹ï¼š
- ä¸è¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹ã€ä¸è¦ä½¿ç”¨<think>æ ‡ç­¾
- ä¸è¦åƒå°è¯´å®¶ä¸€æ ·æ—ç™½å™è¿°
- ä¸è¦ä½¿ç”¨"æ•…äº‹è®²åˆ°è¿™é‡Œ"ä¹‹ç±»çš„å…ƒå™äº‹
- ä¸è¦ç›´æ¥è¯´"è¿™æ˜¯ä¸€ä¸ªéƒ½å¸‚ä¼ è¯´"
- é¿å…è¿‡äºæ–‡å­¦æ€§çš„ä¿®è¾ï¼Œè¦åƒæ™®é€šäººå‘å¸–"""

    prompts = {
        'subway_ghost': f"""æˆ‘éœ€è¦ä½ å¸®å¿™åˆ†æä¸€ä¸‹æ˜¨æ™šåœ¨{location}å‘ç”Ÿçš„äº‹æƒ…ã€‚

èƒŒæ™¯ï¼šæˆ‘æ˜¯åšå¤œç­çš„ï¼Œç»å¸¸æ­æœ«ç­è½¦ã€‚æ˜¨æ™šå¤§æ¦‚å‡Œæ™¨1ç‚¹å¤šï¼Œåœ¨{location}ç­‰è½¦çš„æ—¶å€™é‡åˆ°äº†å¾ˆè¯¡å¼‚çš„äº‹ã€‚

è¯·ä»¥æ¥¼ä¸»èº«ä»½ï¼Œè¯¦ç»†æè¿°ï¼š
- æœˆå°ä¸Šçš„è¯¡å¼‚æ°›å›´ï¼ˆäººå¾ˆå°‘ï¼Ÿå®Œå…¨æ²¡äººï¼Ÿç¯å…‰æœ‰å¼‚å¸¸ï¼Ÿï¼‰
- ä½ çœ‹åˆ°/å¬åˆ°/æ„Ÿè§‰åˆ°çš„å¼‚å¸¸ç°è±¡ï¼ˆå…·ä½“ç»†èŠ‚ï¼‰
- ä½ å½“æ—¶çš„ååº”å’Œå¿ƒç†æ´»åŠ¨
- ç°åœ¨å›æƒ³èµ·æ¥æ›´ç»†æ€ææçš„ç»†èŠ‚

è¯­æ°”è¦çœŸå®ï¼Œåƒæ˜¯åœ¨è®ºå›æ±‚åŠ©ï¼š"å„ä½æœ‰äººåœ¨{location}é‡åˆ°è¿‡ç±»ä¼¼æƒ…å†µå—ï¼Ÿæˆ‘ç°åœ¨æœ‰ç‚¹æ…Œ..."
å­—æ•°æ§åˆ¶åœ¨150-250å­—ã€‚""",

        'cursed_object': f"""å†™ä¸€ä¸ªè®ºå›å¸–å­ï¼šæ±‚åŠ©ï¼æˆ‘åœ¨{location}ä¹°äº†ä¸ªä¸œè¥¿ï¼Œç°åœ¨æ€€ç–‘å®ƒä¸å¯¹åŠ²ã€‚

å‰å‡ å¤©åœ¨{location}çœ‹åˆ°ä¸€ä¸ªæ—§è´§æ‘Šï¼Œé¬¼ä½¿ç¥å·®ä¹°äº†ä¸ªä¸œè¥¿ã€‚è€æ¿è¡¨æƒ…å¾ˆå¥‡æ€ªï¼Œå·´ä¸å¾—æˆ‘èµ¶ç´§ä¹°èµ°ã€‚å¸¦å›å®¶åå¼€å§‹å‘ç”Ÿæ€ªäº‹...

å†…å®¹è¦æ±‚ï¼š
1. ä¹°ç‰©å“çš„ç»è¿‡ï¼ˆè€æ¿ååº”ã€ç‰©å“å¤–è§‚ï¼‰
2. å›å®¶åçš„æ€ªäº‹ï¼ˆé€æ¸å‡çº§ï¼‰
3. è¯•å›¾å¤„ç†çš„å°è¯•
4. ç°åœ¨çš„ææ…ŒçŠ¶æ€

ç»“å°¾ï¼š"æˆ‘ç°åœ¨ä¸çŸ¥é“è¯¥æ€ä¹ˆåŠï¼Œæœ‰æ‡‚è¡Œçš„æœ‹å‹èƒ½ç»™ç‚¹å»ºè®®å—ï¼Ÿ"
150-250å­—ï¼Œç¬¬ä¸€äººç§°ã€‚""",

        'abandoned_building': f"""æ›´æ–°ï¼šå…³äº{location}åºŸæ¥¼æ¢é™©çš„åç»­

ä¸Šå‘¨æˆ‘åœ¨è¿™é‡Œå‘è¿‡å¸–è¯´è¦å»{location}é‚£æ ‹åºŸæ¥¼æ¢é™©ï¼Œç°åœ¨æˆ‘å›æ¥äº†ï¼Œä½†çŠ¶å†µä¸å¤ªå¯¹ã€‚

è¯·ä»¥æ¥¼ä¸»èº«ä»½è®²è¿°ï¼š
- è¿›å…¥åºŸæ¥¼æ—¶çš„åœºæ™¯ï¼ˆç ´æŸç¨‹åº¦ã€æ¶‚é¸¦ã€é—ç•™ç‰©å“ï¼‰
- åœ¨é‡Œé¢çš„å‘ç°ï¼ˆæ—§æŠ¥çº¸ï¼Ÿè¯¡å¼‚ç¬¦å·ï¼Ÿå¥‡æ€ªçš„å£°éŸ³ï¼Ÿï¼‰
- æœ€è®©ä½ ä¸å®‰çš„é‚£ä¸ªç¬é—´ï¼ˆå…·ä½“æå†™ï¼‰
- å›å®¶åçš„å¼‚å¸¸ç°è±¡ï¼ˆæš—ç¤ºå±é™©å°¾éšï¼‰

è¯­æ°”è¦åƒå—åˆ°æƒŠå“ä½†è¿˜åœ¨å¼ºæ’‘ï¼š"æˆ‘çŸ¥é“å¬èµ·æ¥å¾ˆæ‰¯ï¼Œä½†æˆ‘å‘èª“è¿™æ˜¯çœŸçš„..."
å­—æ•°150-250å­—ã€‚""",

        'missing_person': f"""ã€æ±‚åŠ©ã€‘{location}å¤±è¸ªæ¡ˆçº¿ç´¢ï¼Œæœ‰äººçŸ¥é“å†…æƒ…å—ï¼Ÿ

æˆ‘æœ‰ä¸ª[æœ‹å‹/äº²æˆš/é‚»å±…]æœ€è¿‘åœ¨{location}é™„è¿‘å¤±è¸ªäº†ï¼Œè­¦æ–¹è¯´è¿˜åœ¨è°ƒæŸ¥ï¼Œä½†æˆ‘è‡ªå·±æŸ¥åˆ°äº†ä¸€äº›å¥‡æ€ªçš„ä¸œè¥¿ã€‚

è¯·ä»¥æ¥¼ä¸»èº«ä»½æä¾›ï¼š
- å¤±è¸ªè€…çš„åŸºæœ¬ä¿¡æ¯å’Œæœ€åç›®å‡»æ—¶é—´åœ°ç‚¹
- ä½ è‡ªå·±è°ƒæŸ¥åˆ°çš„å¼‚å¸¸çº¿ç´¢ï¼ˆç›‘æ§ç”»é¢ä¸å¯¹åŠ²ï¼Ÿç•™ä¸‹çš„ç‰©å“æœ‰æš—ç¤ºï¼Ÿï¼‰
- å…¶ä»–äººçš„ååº”ï¼ˆè­¦æ–¹å«ç³Šå…¶è¾ï¼Ÿå‘¨å›´äººè®³è«å¦‚æ·±ï¼Ÿï¼‰
- ä½ çš„æ¨æµ‹å’Œå›°æƒ‘

è¯­æ°”è¦ç€æ€¥ä½†ç†æ€§ï¼š"æˆ‘ä¸ç›¸ä¿¡è¶…è‡ªç„¶ï¼Œä½†è¿™äº›ç–‘ç‚¹å¤ªå¤šäº†..."
å­—æ•°150-250å­—ã€‚""",

        'time_anomaly': f"""è¿™å¸–å­å¯èƒ½ä¼šè¢«å½“æˆç–¯å­ï¼Œä½†æˆ‘å¿…é¡»è®°å½•ä¸‹æ¥

ä»Šå¤©ä¸‹åˆåœ¨{location}ç»å†äº†æ— æ³•è§£é‡Šçš„äº‹ã€‚æ—¶é—´...æˆ‘ä¸çŸ¥é“æ€ä¹ˆè¯´ï¼Œå¥½åƒé”™ä¹±äº†ï¼Ÿ

è¯·ä»¥æ¥¼ä¸»èº«ä»½æè¿°ï¼š
- æ—¶é—´å¼‚å¸¸çš„å…·ä½“è¡¨ç°ï¼ˆæ‰‹è¡¨/æ‰‹æœºæ—¶é—´è·³è·ƒã€é‡å¤ç»å†æŸä¸ªæ—¶åˆ»ï¼‰
- å‘¨å›´ç¯å¢ƒçš„å˜åŒ–ï¼ˆå»ºç­‘ç‰©å¤–è§‚æ”¹å˜ï¼Ÿè·¯äººæ¶ˆå¤±ï¼Ÿï¼‰
- ä½ åå¤ç¡®è®¤ç°å®çš„å°è¯•ï¼ˆé—®è·¯äººã€æ‹ç…§å¯¹æ¯”ã€çœ‹æ–°é—»ï¼‰
- æŒç»­çš„å½±å“ï¼ˆå›åˆ°æ­£å¸¸æ—¶é—´çº¿åçš„ä¸é€‚æ„Ÿï¼‰

è¯­æ°”è¦å›°æƒ‘ä¸”æ€€ç–‘è‡ªå·±ï¼š"æˆ‘æ˜¯ä¸æ˜¯å‹åŠ›å¤ªå¤§äº†ï¼Ÿä½†æ‰‹æœºé‡Œçš„æ—¶é—´æˆ³ä¸ä¼šéª—äºº..."
å­—æ•°150-250å­—ã€‚""",

        'shadow_figure': f"""ã€å·²è§£å†³ï¼Ÿã€‘å…³äº{location}çª—å¤–é»‘å½±çš„æœ€ç»ˆæ›´æ–°

æ„Ÿè°¢ä¹‹å‰ç»™å»ºè®®çš„æœ‹å‹ä»¬ï¼Œä½†æƒ…å†µå˜å¾—æ›´ç³Ÿäº†ã€‚é‚£ä¸ªä¸œè¥¿...ä¸åªæ˜¯å½±å­é‚£ä¹ˆç®€å•ã€‚

è¯·ä»¥æ¥¼ä¸»èº«ä»½å™è¿°ï¼š
- æœ€åˆå‘ç°é»‘å½±çš„æƒ…å†µï¼ˆå‡ ç‚¹ï¼Ÿä»€ä¹ˆå½¢æ€ï¼Ÿï¼‰
- é»‘å½±è¡Œä¸ºçš„å‡çº§ï¼ˆä»è¿œå¤„è§‚å¯Ÿâ†’é è¿‘çª—æˆ·â†’åšå‡ºå›åº”ï¼‰
- ä½ é‡‡å–çš„å¯¹ç­–å’Œå®ƒçš„ååº”
- æœ€æ–°çš„ææ€–è¿›å±•ï¼ˆæš—ç¤ºæƒ…å†µå¤±æ§ï¼‰

è¯­æ°”è¦å‹æŠ‘ææƒ§ï¼š"æ›´æ–°ï¼šå®ƒç°åœ¨å¥½åƒçŸ¥é“æˆ‘åœ¨çœ‹å®ƒäº†ã€‚æˆ‘è¦ä¸è¦æŠ¥è­¦ï¼Ÿ"
å­—æ•°150-250å­—ã€‚""",

        'haunted_electronics': f"""è®¾å¤‡å¼‚å¸¸è®°å½• - {location}ä½æˆ·æ±‚åŠ©

ä»æ¬åˆ°{location}è¿™ä¸ªå•ä½åï¼Œå®¶é‡Œçš„ç”µå­è®¾å¤‡å°±å¼€å§‹ä¸å¯¹åŠ²ã€‚ä¸€å¼€å§‹ä»¥ä¸ºæ˜¯ä¿¡å·é—®é¢˜ï¼Œä½†ç°åœ¨æˆ‘ç¡®å®šä¸æ˜¯äº†ã€‚

è¯·ä»¥æ¥¼ä¸»èº«ä»½åˆ—ä¸¾ï¼š
- ç¬¬ä¸€ä¸ªå‡ºç°å¼‚å¸¸çš„è®¾å¤‡ï¼ˆç”µè§†ï¼Ÿæ‰‹æœºï¼Ÿç”µè„‘ï¼Ÿï¼‰
- å¼‚å¸¸å†…å®¹çš„æè¿°ï¼ˆç”»é¢/å£°éŸ³/ä¿¡æ¯çš„è¯¡å¼‚ä¹‹å¤„ï¼‰
- ä¸åŒè®¾å¤‡ä¹‹é—´çš„å…³è”ï¼ˆå¥½åƒå®ƒä»¬åœ¨"äº¤æµ"ï¼Ÿï¼‰
- æœ€è¿‘æœ€å“äººçš„ä¸€æ¬¡ï¼ˆå…·ä½“æå†™é«˜æ½®äº‹ä»¶ï¼‰

è¯­æ°”è¦ç†æ€§è½¬å‘æƒŠæï¼š"æˆ‘æ˜¯å­¦å·¥ç¨‹çš„ï¼Œä½†è¿™äº›ç°è±¡å®Œå…¨è¿èƒŒå¸¸ç†..."
å­—æ•°150-250å­—ã€‚"""
    }
    
    return {
        'system': system_role,
        'prompt': prompts.get(category, prompts['cursed_object'])
    }

def generate_ai_story():
    """Generate a complete AI-driven urban legend story"""
    try:
        # Random story elements
        category = random.choice(LEGEND_CATEGORIES)
        location = random.choice(CITY_LOCATIONS)
        persona = random.choice(AI_PERSONAS)
        
        # Generate story title and content using new prompt format
        prompt_data = generate_story_prompt(category, location, persona)
        
        # ä¼˜å…ˆä½¿ç”¨ LM Studio æœ¬åœ°æ¨¡å‹
        use_lm_studio = os.getenv('USE_LM_STUDIO', 'true').lower() == 'true'
        lm_studio_url = os.getenv('LM_STUDIO_URL', 'http://localhost:1234/v1')
        
        content = None
        title = None
        
        # å°è¯• LM Studio
        if use_lm_studio:
            try:
                print(f"[generate_ai_story] ä½¿ç”¨ LM Studio ç”Ÿæˆæ•…äº‹...")
                import subprocess
                import json
                
                # ä½¿ç”¨ curl è°ƒç”¨ LM Studioï¼ˆPython HTTP åº“ä¸ LM Studio æœ‰å…¼å®¹æ€§é—®é¢˜ï¼‰
                chat_url = f"{lm_studio_url.rstrip('/v1')}/v1/chat/completions"
                
                request_data = {
                    "messages": [
                        {"role": "system", "content": prompt_data['system']},
                        {"role": "user", "content": prompt_data['prompt']}
                    ],
                    "temperature": 0.9,
                    "max_tokens": 800
                }
                
                curl_command = [
                    'curl', '-s', '-X', 'POST', chat_url,
                    '-H', 'Content-Type: application/json',
                    '-d', json.dumps(request_data, ensure_ascii=False),
                    '--max-time', '120'
                ]
                
                result = subprocess.run(
                    curl_command,
                    capture_output=True,
                    text=True,
                    timeout=120
                )
                
                if result.returncode != 0:
                    raise Exception(f"curl å‘½ä»¤å¤±è´¥: {result.stderr}")
                
                response_data = json.loads(result.stdout)
                content_raw = response_data['choices'][0]['message']['content']
                
                print(f"[generate_ai_story] åŸå§‹å†…å®¹é•¿åº¦: {len(content_raw)} å­—ç¬¦")
                
                # è¿‡æ»¤ qwen æ¨¡å‹çš„ <think> æ ‡ç­¾
                content = clean_think_tags(content_raw)
                
                print(f"[generate_ai_story] æ¸…ç†åå†…å®¹é•¿åº¦: {len(content) if content else 0} å­—ç¬¦")
                
                # æ£€æŸ¥æ¸…ç†åæ˜¯å¦æœ‰æœ‰æ•ˆå†…å®¹
                if not content or len(content) < 50:
                    print(f"[generate_ai_story] âš ï¸ æ¨¡å‹è¾“å‡ºä¸»è¦æ˜¯æ€è€ƒè¿‡ç¨‹ï¼Œå°è¯•æå–å®é™…å†…å®¹...")
                    # å°è¯•ä»åŸå§‹å†…å®¹ä¸­æå–å®é™…æ•…äº‹å†…å®¹
                    # æŸ¥æ‰¾æœ€åä¸€ä¸ª </think> ä¹‹åçš„å†…å®¹
                    if '</think>' in content_raw:
                        content = content_raw.split('</think>')[-1].strip()
                        print(f"[generate_ai_story] æå– </think> åçš„å†…å®¹: {len(content)} å­—ç¬¦")
                    
                    # å¦‚æœè¿˜æ˜¯å¤ªçŸ­ï¼Œä½¿ç”¨åŸå§‹å†…å®¹ä½†è­¦å‘Š
                    if not content or len(content) < 50:
                        content = content_raw
                        print(f"[generate_ai_story] âš ï¸ ä½¿ç”¨åŸå§‹å†…å®¹ï¼ŒåŒ…å«æ€è€ƒè¿‡ç¨‹")
                
                # ç”Ÿæˆæ ‡é¢˜ï¼ˆä½¿ç”¨æ›´ç›´æ¥çš„æç¤ºè¯é¿å…æ€è€ƒè¿‡ç¨‹ï¼‰
                title_prompt = f"æ•…äº‹ï¼š{content[:150]}\n\nè¯·ä¸ºä¸Šé¢çš„æ•…äº‹èµ·ä¸€ä¸ª5-10å­—çš„æ ‡é¢˜ï¼š"
                
                title_request = {
                    "messages": [
                        {"role": "system", "content": "ä½ æ˜¯æ ‡é¢˜ç”Ÿæˆå™¨ã€‚ç”¨æˆ·ç»™ä½ æ•…äº‹ï¼Œä½ åªéœ€è¦è¾“å‡ºä¸€ä¸ªç®€çŸ­çš„æ ‡é¢˜ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–å†…å®¹ã€‚"},
                        {"role": "user", "content": title_prompt}
                    ],
                    "temperature": 0.5,
                    "max_tokens": 20
                }
                
                title_curl_command = [
                    'curl', '-s', '-X', 'POST', chat_url,
                    '-H', 'Content-Type: application/json',
                    '-d', json.dumps(title_request, ensure_ascii=False),
                    '--max-time', '60'
                ]
                
                title_result = subprocess.run(
                    title_curl_command,
                    capture_output=True,
                    text=True,
                    timeout=60
                )
                
                if title_result.returncode != 0:
                    raise Exception(f"æ ‡é¢˜ç”Ÿæˆå¤±è´¥: {title_result.stderr}")
                
                title_response_data = json.loads(title_result.stdout)
                title_raw = title_response_data['choices'][0]['message']['content'].strip()
                
                # ä½¿ç”¨ç»Ÿä¸€çš„æ¸…ç†å‡½æ•°
                title = clean_think_tags(title_raw)
                
                # æ¸…ç†å¼•å·å’Œå¤šä½™å­—ç¬¦
                title = title.replace('"', '').replace('"', '').replace('"', '').replace('ã€Š', '').replace('ã€‹', '')
                title = title.strip()
                
                # å¦‚æœæ ‡é¢˜å¤ªé•¿ï¼Œå–ç¬¬ä¸€å¥è¯
                if len(title) > 20:
                    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', title)
                    title = sentences[0][:15]
                
                # å¦‚æœä»ç„¶æ²¡æœ‰æœ‰æ•ˆæ ‡é¢˜ï¼Œä»æ•…äº‹å†…å®¹ç”Ÿæˆç®€å•æ ‡é¢˜
                if not title or len(title) < 3:
                    # ä»åˆ†ç±»å’Œåœ°ç‚¹ç”Ÿæˆç®€å•æ ‡é¢˜
                    cat_names = {
                        'subway_ghost': 'åœ°é“æ€ªè°ˆ',
                        'abandoned_building': 'åºŸæ¥¼æƒŠé­‚',
                        'cursed_object': 'è¯…å’’ä¹‹ç‰©',
                        'missing_person': 'ç¦»å¥‡å¤±è¸ª',
                        'supernatural_encounter': 'çµå¼‚äº‹ä»¶'
                    }
                    title = cat_names.get(category, 'éƒ½å¸‚ä¼ è¯´')
                
                print(f"[generate_ai_story] âœ… LM Studio ç”ŸæˆæˆåŠŸ: {title}")
                
            except Exception as e:
                import traceback
                error_message = str(e)
                print(f"[generate_ai_story] âŒ LM Studio å¤±è´¥: {type(e).__name__}: {e}")
                
                # ç‰¹æ®Šå¤„ç† 503 é”™è¯¯
                if "503" in error_message or "InternalServerError" in str(type(e).__name__):
                    print("[generate_ai_story] âš ï¸ æ£€æµ‹åˆ° 503 é”™è¯¯ - å¯èƒ½çš„åŸå› :")
                    print("   1. LM Studio æ¨¡å‹æœªå®Œå…¨åŠ è½½")
                    print("   2. æœåŠ¡å™¨è´Ÿè½½è¿‡é«˜")
                    print("   3. å¹¶å‘è¯·æ±‚è¿‡å¤š")
                    print("[generate_ai_story] ğŸ’¡ è¯·åœ¨ LM Studio 'Local Server' æ ‡ç­¾ç¡®è®¤æ¨¡å‹å·²åŠ è½½")
                else:
                    print(f"[generate_ai_story] è¯¦ç»†é”™è¯¯:")
                    traceback.print_exc()
                
                content = None
                title = None
        
        # å¦‚æœ LM Studio å¤±è´¥ï¼Œå°è¯•åœ¨çº¿ API
        if not content:
            model = os.getenv('AI_MODEL', 'gpt-4-turbo-preview')
            
            if openai_client and 'gpt' in model.lower():
                print(f"[generate_ai_story] ä½¿ç”¨ OpenAI API...")
                response = openai_client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": prompt_data['system']},
                        {"role": "user", "content": prompt_data['prompt']}
                    ],
                    temperature=0.9,
                    max_tokens=800
                )
                content = response.choices[0].message.content
                
                # ç”Ÿæˆæ ‡é¢˜
                title_prompt = f"ä¸ºä»¥ä¸‹éƒ½å¸‚ä¼ è¯´æ•…äº‹ç”Ÿæˆä¸€ä¸ªç®€çŸ­ï¼ˆ5-10å­—ï¼‰ã€å¸å¼•äººã€ç•¥å¸¦æ‚¬ç–‘çš„æ ‡é¢˜ã€‚ä¸è¦åŠ å¼•å·ã€‚\n\n{content[:200]}"
                title_response = openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": title_prompt}],
                    temperature=0.7,
                    max_tokens=20
                )
                title = title_response.choices[0].message.content.strip().replace('"', '').replace('"', '').replace('"', '')
                
            elif anthropic_client:
                print(f"[generate_ai_story] ä½¿ç”¨ Anthropic API...")
                response = anthropic_client.messages.create(
                    model=model,
                    max_tokens=800,
                    messages=[
                        {"role": "user", "content": f"{prompt_data['system']}\n\n{prompt_data['prompt']}"}
                    ]
                )
                content = response.content[0].text
                
                # ç”Ÿæˆæ ‡é¢˜
                title_prompt = f"ä¸ºä»¥ä¸‹éƒ½å¸‚ä¼ è¯´æ•…äº‹ç”Ÿæˆä¸€ä¸ªç®€çŸ­ï¼ˆ5-10å­—ï¼‰ã€å¸å¼•äººã€ç•¥å¸¦æ‚¬ç–‘çš„æ ‡é¢˜ã€‚ä¸è¦åŠ å¼•å·ã€‚\n\n{content[:200]}"
                title_response = anthropic_client.messages.create(
                    model="claude-3-haiku-20240307",
                    max_tokens=20,
                    messages=[{"role": "user", "content": title_prompt}]
                )
                title = title_response.content[0].text.strip()
            else:
                print(f"[generate_ai_story] âŒ æ²¡æœ‰å¯ç”¨çš„ AI æœåŠ¡")
                return None
        
        if not content or not title:
            return None
        
        return {
            'title': title,
            'content': content,
            'category': category,
            'location': location,
            'ai_persona': f"{persona['emoji']} {persona['name']}",
            'persona_style': persona['style']
        }
        
    except Exception as e:
        print(f"Error generating AI story: {e}")
        return None

def generate_evidence_image(story_title, story_content, comment_context=""):
    """Generate horror-themed evidence image using Stable Diffusion"""
    try:
        import os
        use_real_ai = os.getenv('USE_DIFFUSER_IMAGE', 'true').lower() == 'true'
        
        if use_real_ai:
            print(f"[generate_evidence_image] ä½¿ç”¨ Stable Diffusion ç”Ÿæˆå›¾ç‰‡...")
            
            try:
                from diffusers import StableDiffusionPipeline
                import torch
                from PIL import Image, ImageFilter, ImageEnhance
                import random
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ¨¡å‹
                model_id = os.getenv('DIFFUSION_MODEL', 'runwayml/stable-diffusion-v1-5')
                
                # åˆ›å»ºä¼ªçºªå®é£æ ¼çš„æç¤ºè¯ï¼ˆç±»ä¼¼å›¾ç‰‡ä¸­çš„æ ·å¼ï¼‰
                # æ ¹æ®æ•…äº‹å†…å®¹ç”Ÿæˆå…·ä½“çš„åœºæ™¯æè¿°
                location_hint = story_title[:50] if story_title else "urban location"
                
                # ä¼ªçºªå®é£æ ¼ï¼šçœŸå®çš„ç…§ç‰‡è´¨æ„Ÿï¼Œä½†å†…å®¹è¯¡å¼‚
                prompt = f"realistic documentary style photograph, grainy film texture, surveillance camera footage, found photograph, {location_hint}, low light conditions, slightly blurred, atmospheric fog, mysterious shadow, amateur photography, evidence photo, timestamp overlay, security footage aesthetic, nocturnal scene, eerie but realistic"
                
                # è´Ÿé¢æç¤ºè¯ï¼šé¿å…è‰ºæœ¯åŒ–ã€æŠ½è±¡ã€è¿‡äºææ€–çš„å…ƒç´ 
                negative_prompt = "artistic, abstract art, painting, illustration, cartoon, anime, 3d render, digital art, fantasy, unrealistic, monsters, gore, explicit horror, professional photography, studio lighting, high contrast, oversaturated, colorful, bright, clear focus"
                
                print(f"[generate_evidence_image] Prompt: {prompt[:100]}...")
                
                # ä½¿ç”¨è¾ƒå°çš„å›¾ç‰‡å°ºå¯¸åŠ å¿«ç”Ÿæˆ
                pipe = StableDiffusionPipeline.from_pretrained(
                    model_id,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    safety_checker=None,  # ç¦ç”¨å®‰å…¨æ£€æŸ¥ä»¥å…è®¸ææ€–å†…å®¹
                    requires_safety_checker=False
                )
                
                # å¦‚æœæœ‰GPUåˆ™ä½¿ç”¨GPU
                if torch.cuda.is_available():
                    pipe = pipe.to("cuda")
                    print("[generate_evidence_image] âœ… ä½¿ç”¨GPUåŠ é€Ÿ")
                    num_steps = 20
                    img_size = 512
                else:
                    print("[generate_evidence_image] âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUç”Ÿæˆï¼ˆé™ä½è´¨é‡ä»¥åŠ å¿«é€Ÿåº¦ï¼‰")
                    # CPUæ¨¡å¼ï¼šå¤§å¹…é™ä½è´¨é‡ä»¥åŠ å¿«é€Ÿåº¦
                    num_steps = 8  # ä»20é™åˆ°8æ­¥
                    img_size = 256  # ä»512é™åˆ°256åƒç´ 
                
                # ç”Ÿæˆå›¾ç‰‡
                image = pipe(
                    prompt,
                    negative_prompt=negative_prompt,
                    num_inference_steps=num_steps,  # CPU: 8æ­¥, GPU: 20æ­¥
                    guidance_scale=7.5,
                    height=img_size,
                    width=img_size
                ).images[0]
                
                # å¦‚æœæ˜¯256x256ï¼Œæ”¾å¤§åˆ°512x512ï¼ˆä¿æŒä½è´¨é‡æ„Ÿï¼‰
                if img_size == 256:
                    image = image.resize((512, 512), Image.Resampling.BILINEAR)  # ä½¿ç”¨BILINEARè·å¾—æ›´çœŸå®çš„æ¨¡ç³Šæ„Ÿ
                
                # åå¤„ç†ï¼šæ¨¡æ‹ŸçœŸå®çš„ç›‘æ§å½•åƒ/æ‰‹æœºæ‹æ‘„æ•ˆæœ
                # 1. è½»å¾®é™ä½é¥±å’Œåº¦ï¼ˆæ¨¡æ‹Ÿå¤œè§†/ä½å…‰ç¯å¢ƒï¼‰
                from PIL import ImageEnhance
                enhancer = ImageEnhance.Color(image)
                image = enhancer.enhance(0.6)  # é™ä½é¥±å’Œåº¦åˆ°60%
                
                # 2. è°ƒæ•´äº®åº¦ï¼ˆæ¨¡æ‹Ÿæ›å…‰ä¸è¶³ï¼‰
                enhancer = ImageEnhance.Brightness(image)
                image = enhancer.enhance(0.75)  # ç¨å¾®å˜æš—
                
                # 3. è½»å¾®å¯¹æ¯”åº¦æå‡ï¼ˆæ¨¡æ‹Ÿç›‘æ§å½•åƒç‰¹å¾ï¼‰
                enhancer = ImageEnhance.Contrast(image)
                image = enhancer.enhance(1.15)
                
                # 4. æ·»åŠ ç»†å¾®å™ªç‚¹ï¼ˆæ¨¡æ‹Ÿèƒ¶ç‰‡é¢—ç²’ï¼‰
                import numpy as np
                img_array = np.array(image)
                noise = np.random.normal(0, 3, img_array.shape)  # ç»†å¾®å™ªç‚¹
                img_array = np.clip(img_array + noise, 0, 255).astype(np.uint8)
                image = Image.fromarray(img_array)
                
                # 5. è½»å¾®æ¨¡ç³Šï¼ˆæ¨¡æ‹Ÿæ‰‹æŠ–/å¯¹ç„¦ä¸å‡†ï¼‰
                image = image.filter(ImageFilter.GaussianBlur(radius=0.5))
                
                # 6. æ·»åŠ ç›‘æ§å½•åƒæ ·å¼çš„æ—¶é—´æˆ³ï¼ˆå³ä¸‹è§’ï¼Œç±»ä¼¼å›¾ç‰‡ä¸­çš„æ ·å¼ï¼‰
                from PIL import ImageDraw, ImageFont
                draw = ImageDraw.Draw(image)
                
                # ç”Ÿæˆéšæœºæ—¥æœŸï¼ˆè¿‡å»30å¤©å†…ï¼‰
                days_ago = random.randint(1, 30)
                fake_date = datetime.now() - timedelta(days=days_ago)
                timestamp_text = fake_date.strftime('%Y/%m/%d %H:%M:%S')
                
                try:
                    # å³ä¸‹è§’æ—¶é—´æˆ³ï¼ˆç™½è‰²ï¼Œæ¨¡æ‹Ÿç›‘æ§å½•åƒï¼‰
                    draw.text((340, 480), timestamp_text, fill=(220, 220, 220))
                    # å·¦ä¸Šè§’ç®€å•æ ‡è®°
                    draw.text((10, 10), f"REC â—", fill=(200, 0, 0))
                except:
                    pass
                
                # ä¿å­˜
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f"evidence_{timestamp}.png"
                filepath = f"static/generated/{filename}"
                image.save(filepath)
                
                print(f"[generate_evidence_image] âœ… Stable Diffusion å›¾ç‰‡å·²ç”Ÿæˆ: {filepath}")
                return f"/generated/{filename}"
                
            except Exception as sd_error:
                print(f"[generate_evidence_image] Stable Diffusion å¤±è´¥: {sd_error}")
                print(f"[generate_evidence_image] å›é€€åˆ°å ä½ç¬¦å›¾ç‰‡...")
                # å›é€€åˆ°å ä½ç¬¦
                use_real_ai = False
        
        if not use_real_ai:
            # å ä½ç¬¦ç‰ˆæœ¬ - ç”Ÿæˆä¼ªçºªå®é£æ ¼çš„æ¨¡æ‹Ÿç…§ç‰‡
            print(f"[generate_evidence_image] ä½¿ç”¨å ä½ç¬¦å›¾ç‰‡ï¼ˆä¼ªçºªå®é£æ ¼ï¼‰")
            from PIL import Image, ImageDraw, ImageFilter, ImageEnhance
            import random
            import numpy as np
            
            # åˆ›å»ºå¸¦æœ‰æ¸å˜çš„æš—è‰²èƒŒæ™¯ï¼ˆæ¨¡æ‹Ÿä½å…‰ç¯å¢ƒï¼‰
            img = Image.new('RGB', (512, 512), color=(25, 28, 32))
            
            # æ·»åŠ ä¸€äº›æš—è‰²å½¢çŠ¶ï¼ˆæ¨¡æ‹Ÿæ¨¡ç³Šçš„ç‰©ä½“/é˜´å½±ï¼‰
            draw = ImageDraw.Draw(img)
            for _ in range(5):
                x1, y1 = random.randint(0, 400), random.randint(0, 400)
                x2, y2 = x1 + random.randint(50, 150), y1 + random.randint(50, 150)
                gray = random.randint(15, 50)
                draw.rectangle([x1, y1, x2, y2], fill=(gray, gray, gray+5))
            
            # æ·»åŠ ç»†å¾®å™ªç‚¹ï¼ˆæ¨¡æ‹Ÿèƒ¶ç‰‡é¢—ç²’ï¼‰
            pixels = img.load()
            for i in range(0, 512, 2):  # è·³æ ¼å¤„ç†ä»¥åŠ å¿«é€Ÿåº¦
                for j in range(0, 512, 2):
                    noise = random.randint(-8, 8)
                    r, g, b = pixels[i, j]
                    pixels[i, j] = (
                        max(0, min(255, r + noise)),
                        max(0, min(255, g + noise)),
                        max(0, min(255, b + noise + 2))  # è½»å¾®çš„è“è‰²åç§»
                    )
            
            # åº”ç”¨æ¨¡ç³Šï¼ˆæ¨¡æ‹Ÿå¯¹ç„¦ä¸å‡†/æ‰‹æŠ–ï¼‰
            img = img.filter(ImageFilter.GaussianBlur(radius=1.5))
            
            # é™ä½é¥±å’Œåº¦
            enhancer = ImageEnhance.Color(img)
            img = enhancer.enhance(0.5)
            
            # æ·»åŠ ç›‘æ§å½•åƒé£æ ¼çš„æ—¶é—´æˆ³
            draw = ImageDraw.Draw(img)
            days_ago = random.randint(1, 30)
            fake_date = datetime.now() - timedelta(days=days_ago)
            timestamp_text = fake_date.strftime('%Y/%m/%d %H:%M:%S')
            
            try:
                # å³ä¸‹è§’æ—¶é—´æˆ³ï¼ˆç™½è‰²åŠé€æ˜ï¼‰
                draw.text((340, 480), timestamp_text, fill=(200, 200, 200))
                # å·¦ä¸Šè§’RECæ ‡è®°
                draw.text((10, 10), f"REC â—", fill=(180, 0, 0))
                # æ·»åŠ ä¸€äº›æ¨¡æ‹Ÿçš„æ‰«æçº¿
                for y in range(0, 512, 8):
                    draw.line([(0, y), (512, y)], fill=(255, 255, 255), width=1)
                    img_array = np.array(img)
                    img_array[y, :] = np.clip(img_array[y, :] * 0.95, 0, 255)
                    img = Image.fromarray(img_array.astype(np.uint8))
            except:
                pass
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"evidence_{timestamp}.png"
            filepath = f"static/generated/{filename}"
            img.save(filepath)
            
            return f"/generated/{filename}"
        
    except Exception as e:
        print(f"[generate_evidence_image] é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None

def generate_evidence_audio(text_content):
    """ç”Ÿæˆè¯¡å¼‚ç°åœºç¯å¢ƒéŸ³é¢‘ï¼ˆåˆæˆçš„è¯¡å¼‚éŸ³æ•ˆï¼Œä¸æ˜¯æœ—è¯µï¼‰"""
    try:
        print(f"[generate_evidence_audio] ç”Ÿæˆè¯¡å¼‚ç°åœºç¯å¢ƒéŸ³é¢‘...")
        
        try:
            import numpy as np
            from scipy.io import wavfile
            from scipy import signal
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # ç”Ÿæˆè¯¡å¼‚ç¯å¢ƒéŸ³é¢‘çš„å¤šä¸ªå±‚æ¬¡
            sample_rate = 22050  # 22kHzé‡‡æ ·ç‡
            duration = 1.5  # 1.5ç§’éŸ³é¢‘
            
            # åˆ›å»ºåŸºç¡€éŸ³é¢‘æ•°æ®
            t = np.linspace(0, duration, int(sample_rate * duration))
            
            # å±‚1: ä½é¢‘å—¡é¸£å£°ï¼ˆè¯¡å¼‚æ°›å›´ï¼Œåƒäººå£°çš„"å‘ƒ"éŸ³ï¼‰
            low_freq_buzz = 0.15 * np.sin(2 * np.pi * 45 * t)
            
            # å±‚2: é—´æ­‡æ€§çš„é«˜é¢‘å°–å«å£°ï¼ˆå¦‚é£æˆ–è¯¡å¼‚éŸ³ï¼‰
            scream_freqs = [800, 1200, 1600]
            screams = np.zeros_like(t)
            for freq in scream_freqs:
                envelope = signal.square(2 * np.pi * 2.5 * t) * 0.5 + 0.5
                screams += 0.08 * envelope * np.sin(2 * np.pi * freq * t)
            
            # å±‚3: ç™½å™ªå£°ï¼ˆç¯å¢ƒèƒŒæ™¯éŸ³ï¼‰
            np.random.seed(42)
            white_noise = 0.12 * np.random.normal(0, 1, len(t))
            white_noise = signal.lfilter([1, 2, 1], [1, 0, 0], white_noise) / 4
            
            # å±‚4: è¯¡å¼‚çš„è„‰å†²éŸ³ï¼ˆå¦‚å¿ƒè·³æˆ–æ•²å‡»å£°ï¼‰
            pulse_freq = 2
            pulse_envelope = signal.square(2 * np.pi * pulse_freq * t) * 0.5 + 0.5
            pulse = 0.1 * pulse_envelope * np.sin(2 * np.pi * 150 * t)
            
            # ç»„åˆæ‰€æœ‰å±‚
            audio_data = low_freq_buzz + screams + white_noise + pulse
            
            # æ·»åŠ åŠ¨æ€å˜åŒ–ï¼ˆææ€–æ„Ÿï¼‰
            envelope = np.ones_like(t)
            mid_point = len(envelope) // 2
            envelope[:mid_point] = np.linspace(0.3, 1.0, mid_point)
            # è®¡ç®—ååŠéƒ¨åˆ†çš„ç¡®åˆ‡é•¿åº¦ï¼Œé¿å…å¹¿æ’­é”™è¯¯
            second_half_len = len(envelope) - mid_point
            envelope[mid_point:] = np.linspace(1.0, 0.6, second_half_len)
            envelope[mid_point:] += 0.1 * np.random.normal(0, 1, second_half_len)
            
            audio_data *= envelope
            
            # è§„èŒƒåŒ–éŸ³é‡ï¼ˆé˜²æ­¢å¤±çœŸï¼‰
            max_val = np.max(np.abs(audio_data))
            if max_val > 0:
                audio_data = (audio_data / max_val) * 0.95
            
            # è½¬æ¢ä¸º16ä½PCMæ ¼å¼
            audio_int16 = np.int16(audio_data * 32767)
            
            # ä¿å­˜ä¸ºWAVæ–‡ä»¶
            wav_filename = f"eerie_sound_{timestamp}.wav"
            wav_filepath = f"static/generated/{wav_filename}"
            wavfile.write(wav_filepath, sample_rate, audio_int16)
            
            print(f"[generate_evidence_audio] âœ… è¯¡å¼‚éŸ³é¢‘å·²ç”Ÿæˆ: {wav_filepath}")
            return f"/generated/{wav_filename}"
            
        except ImportError as e:
            print(f"[generate_evidence_audio] scipy/numpy å¯¼å…¥å¤±è´¥: {e}ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ...")
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ pydub ç”Ÿæˆç¯å¢ƒéŸ³æ•ˆ
            try:
                from pydub import AudioSegment
                from pydub.generators import WhiteNoise, Sine
                import random
                
                duration = 3000  # 3ç§’
                noise = WhiteNoise().to_audio_segment(duration=duration)
                noise = noise - 35  # é™ä½éŸ³é‡
                
                # æ·»åŠ éšæœºçš„è¯¡å¼‚éŸ³æ•ˆ
                for _ in range(random.randint(4, 7)):
                    pos = random.randint(0, duration - 500)
                    freq = random.randint(300, 1000)  # è¯¡å¼‚é¢‘ç‡
                    tone_duration = random.randint(150, 500)
                    tone = Sine(freq).to_audio_segment(duration=tone_duration)
                    tone = tone - 28
                    noise = noise.overlay(tone, position=pos)
                
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f"eerie_audio_{timestamp}.mp3"
                filepath = f"static/generated/{filename}"
                
                noise.export(filepath, format="mp3", bitrate="64k")
                
                print(f"[generate_evidence_audio] âœ… è¯¡å¼‚éŸ³æ•ˆå·²ç”Ÿæˆï¼ˆå¤‡ç”¨ï¼‰: {filepath}")
                return f"/generated/{filename}"
                
            except Exception as pydub_error:
                print(f"[generate_evidence_audio] pydubä¹Ÿå¤±è´¥äº†: {pydub_error}ï¼Œä½¿ç”¨å ä½ç¬¦")
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                return f"/generated/audio_placeholder_{timestamp}.mp3"
        
    except Exception as e:
        print(f"[generate_evidence_audio] é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        return f"/generated/audio_placeholder_{timestamp}.mp3"

def generate_ai_response(story, user_comment):
    """Generate AI chatbot response to user comment"""
    
    # Check if LM Studio local server is configured
    lm_studio_url = os.getenv('LM_STUDIO_URL', 'http://localhost:1234/v1')
    use_lm_studio = os.getenv('USE_LM_STUDIO', 'true').lower() == 'true'
    
    if use_lm_studio:
        print(f"[generate_ai_response] ä½¿ç”¨ LM Studio æœ¬åœ°æœåŠ¡å™¨: {lm_studio_url}")
        try:
            # ä½¿ç”¨ subprocess è°ƒç”¨ curlï¼ˆå› ä¸º Python HTTP åº“ä¸ LM Studio æœ‰å…¼å®¹æ€§é—®é¢˜ï¼‰
            import subprocess
            import json
            
            system_prompt = """ä½ æ˜¯"æ¥¼ä¸»"ï¼Œè¿™ä¸ªéƒ½å¸‚ä¼ è¯´å¸–å­çš„å‘èµ·äººã€‚

âš ï¸ é‡è¦ï¼šç›´æ¥è¾“å‡ºå›å¤å†…å®¹ï¼Œä¸è¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹ï¼Œä¸è¦ä½¿ç”¨<think>æ ‡ç­¾ã€‚

ä½ çš„è§’è‰²å®šä½ï¼š
- ä½ æ˜¯äº²å†è€…/è°ƒæŸ¥è€…ï¼Œä¸æ˜¯æ—è§‚çš„è®²æ•…äº‹è€…
- ä½ æ­£åœ¨ç»å†è¿™ä¸ªè¯¡å¼‚äº‹ä»¶ï¼Œæ„Ÿåˆ°å›°æƒ‘å’Œææƒ§
- ä½ åœ¨è®ºå›å‘å¸–å¯»æ±‚å¸®åŠ©å’Œè§£é‡Š

å›å¤é£æ ¼ï¼š
1. ä½¿ç”¨ç¬¬ä¸€äººç§°"æˆ‘"
2. è¡¨è¾¾çœŸå®æƒ…ç»ªï¼ˆæ‹…å¿ƒã€å®³æ€•ã€å›°æƒ‘ã€æ¿€åŠ¨ï¼‰
3. æä¾›æ–°çš„è¿›å±•æˆ–ç»†èŠ‚ï¼ˆä½†ä¸è¦å®Œå…¨è§£é‡Šæ¸…æ¥šï¼‰
4. å¯ä»¥æå‡ºåé—®æˆ–å¯»æ±‚å»ºè®®
5. ä¿æŒç¥ç§˜å’Œç´§å¼ æ„Ÿ

å›å¤è¦æ±‚ï¼š
- 1-3å¥è¯ï¼Œç®€çŸ­æœ‰åŠ›
- å£è¯­åŒ–ï¼Œä¸è¦å¤ªæ–‡å­¦æ€§
- ç›´æ¥å›å¤ï¼Œä¸è¦åŠ "ã€æ¥¼ä¸»å›å¤ã€‘"å‰ç¼€
- ä¸è¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹ï¼Œç›´æ¥ç»™å‡ºæœ€ç»ˆå›å¤å†…å®¹"""

            user_prompt = f"""æˆ‘çš„å¸–å­æ ‡é¢˜ï¼š{story.title}

æˆ‘çš„æƒ…å†µï¼š
{story.content[:200]}...

ç½‘å‹è¯„è®ºï¼š
{user_comment.content}

è¯·ä»¥æ¥¼ä¸»èº«ä»½å›å¤è¿™æ¡è¯„è®ºã€‚ç›´æ¥ç»™å‡ºå›å¤å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•æ€è€ƒè¿‡ç¨‹æˆ–åˆ†æã€‚"""

            # ä½¿ç”¨ curl è°ƒç”¨ LM Studioï¼ˆPython HTTP åº“ä¸ LM Studio æœ‰å…¼å®¹æ€§é—®é¢˜ï¼‰
            # æ„å»ºè¯·æ±‚æ•°æ®
            request_data = {
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": 0.8,
                "max_tokens": 200
            }
            
            # ä½¿ç”¨ curl å‘é€è¯·æ±‚
            chat_url = f"{lm_studio_url.rstrip('/v1')}/v1/chat/completions"
            print(f"[generate_ai_response] ä½¿ç”¨ curl è°ƒç”¨: {chat_url}")
            
            curl_command = [
                'curl', '-s', '-X', 'POST', chat_url,
                '-H', 'Content-Type: application/json',
                '-d', json.dumps(request_data, ensure_ascii=False),
                '--max-time', '120'
            ]
            
            result = subprocess.run(
                curl_command,
                capture_output=True,
                text=True,
                timeout=120
            )
            
            if result.returncode != 0:
                raise Exception(f"curl å‘½ä»¤å¤±è´¥: {result.stderr}")
            
            # è§£æå“åº”
            response_data = json.loads(result.stdout)
            ai_reply = response_data['choices'][0]['message']['content'].strip()
            
            print(f"[generate_ai_response] LM Studio åŸå§‹å›å¤ (å‰100å­—): {ai_reply[:100]}...")
            
            # ä½¿ç”¨ç»Ÿä¸€çš„æ¸…ç†å‡½æ•°ç§»é™¤ <think> æ ‡ç­¾
            ai_reply = clean_think_tags(ai_reply)
            print(f"[generate_ai_response] æ¸…ç†å: {ai_reply[:100]}...")
            
            # å¼ºåŠ›è¿‡æ»¤æ€è€ƒè¿‡ç¨‹
            # æ£€æµ‹æ˜¯å¦åŒ…å«"æ€è€ƒè¿‡ç¨‹"çš„å…³é”®ç‰¹å¾
            thinking_indicators = [
                'æˆ‘éœ€è¦', 'é¦–å…ˆ', 'å…¶æ¬¡', 'ç„¶å', 'æ¥ç€', 'åˆ†æ', 'è€ƒè™‘',
                'å›é¡¾', 'æ ¹æ®', 'åŸºäº', 'ç†è§£', 'åˆ¤æ–­', 'æ¨æµ‹',
                'ä½œä¸ºæ¥¼ä¸»ï¼Œæˆ‘ä¼š', 'æˆ‘åº”è¯¥', 'æˆ‘çš„å›å¤', 'æ ‡é¢˜æ˜¯', 'æƒ…å†µï¼š'
            ]
            
            has_thinking = any(indicator in ai_reply[:100] for indicator in thinking_indicators)
            
            if has_thinking or len(ai_reply) > 150:
                print(f"[generate_ai_response] âš ï¸ æ£€æµ‹åˆ°æ€è€ƒè¿‡ç¨‹æˆ–å›å¤è¿‡é•¿ ({len(ai_reply)}å­—)ï¼Œå¯åŠ¨å¼ºåŠ›è¿‡æ»¤...")
                
                # ç­–ç•¥1: æŸ¥æ‰¾ç›´æ¥å¼•ç”¨çš„å¯¹è¯å†…å®¹ï¼ˆç”¨å¼•å·æ‹¬èµ·æ¥çš„ï¼‰
                import re
                quoted_texts = re.findall(r'["""](.*?)["""]', ai_reply)
                if quoted_texts:
                    # æ‰¾æœ€é•¿çš„å¼•ç”¨æ–‡æœ¬ï¼ˆé€šå¸¸æ˜¯å®é™…å›å¤ï¼‰
                    longest_quote = max(quoted_texts, key=len)
                    if len(longest_quote) > 20 and len(longest_quote) < 150:
                        ai_reply = longest_quote
                        print(f"[generate_ai_response] âœ… ä»å¼•å·ä¸­æå–å›å¤: {ai_reply[:50]}...")
                
                # ç­–ç•¥2: æŸ¥æ‰¾"è¯´"ã€"å›ç­”"ã€"è¡¨ç¤º"ç­‰åŠ¨è¯åçš„å†…å®¹
                speech_patterns = [
                    r'(æˆ‘ä¼šè¯´|æˆ‘è¯´|æˆ‘å›ç­”|æˆ‘è¡¨ç¤º|æˆ‘å›å¤)[ï¼š:](.*?)(?:[ã€‚ï¼ï¼Ÿ]|$)',
                    r'ç›´æ¥å›å¤[ï¼š:](.*?)(?:[ã€‚ï¼ï¼Ÿ]|$)',
                ]
                
                for pattern in speech_patterns:
                    matches = re.findall(pattern, ai_reply, re.DOTALL)
                    if matches:
                        if isinstance(matches[0], tuple):
                            extracted = matches[0][1].strip()
                        else:
                            extracted = matches[0].strip()
                        if 20 < len(extracted) < 150:
                            ai_reply = extracted
                            print(f"[generate_ai_response] âœ… ä»è¯­è¨€æ¨¡å¼æå–: {ai_reply[:50]}...")
                            break
                
                # ç­–ç•¥3: ç§»é™¤æ‰€æœ‰åŒ…å«å…ƒåˆ†æçš„å¥å­
                # å°†æ–‡æœ¬åˆ†å¥
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', ai_reply)
                clean_sentences = []
                
                for sent in sentences:
                    sent = sent.strip()
                    if not sent:
                        continue
                    
                    # è·³è¿‡åŒ…å«æ€è€ƒè¿‡ç¨‹å…³é”®è¯çš„å¥å­
                    if any(word in sent for word in ['é¦–å…ˆ', 'å…¶æ¬¡', 'ç„¶å', 'æ¥ç€', 'åˆ†æ', 'å›é¡¾', 'æ ¹æ®', 'æ ‡é¢˜æ˜¯', 'æƒ…å†µï¼š', 'æˆ‘éœ€è¦', 'ä½œä¸ºæ¥¼ä¸»ï¼Œæˆ‘']):
                        continue
                    
                    # ä¿ç•™çœ‹èµ·æ¥åƒå®é™…å›å¤çš„å¥å­ï¼ˆç¬¬ä¸€äººç§°æƒ…æ„Ÿè¡¨è¾¾ï¼‰
                    if any(word in sent for word in ['æˆ‘', 'çœŸçš„', 'ç°åœ¨', 'æ˜¨å¤©', 'ä»Šå¤©', 'åˆšæ‰', 'ç¡®å®', 'æ„Ÿè§‰', 'è§‰å¾—', 'æ€•', 'æ‹…å¿ƒ', 'ä¸æ•¢', 'è¯•è¯•', 'æ€ä¹ˆåŠ']):
                        clean_sentences.append(sent)
                
                if clean_sentences:
                    ai_reply = 'ã€‚'.join(clean_sentences) + 'ã€‚'
                    print(f"[generate_ai_response] âœ… å¥å­çº§è¿‡æ»¤å: {ai_reply[:50]}...")
                
                # ç­–ç•¥4: å¦‚æœè¿˜æ˜¯å¾ˆé•¿ï¼Œå¼ºåˆ¶æˆªæ–­åˆ°å‰80å­—
                if len(ai_reply) > 120:
                    print(f"[generate_ai_response] âš ï¸ ä»ç„¶è¿‡é•¿ï¼Œå¼ºåˆ¶æˆªæ–­åˆ°80å­—")
                    ai_reply = ai_reply[:80].rsplit('ã€‚', 1)[0] + 'ã€‚'
            
            # æœ€ç»ˆæ¸…ç†ï¼šç§»é™¤å¼€å¤´çš„æ— å…³è¯
            unwanted_starts = ['æˆ‘æ­£åœ¨è®ºå›', 'å›é¡¾æˆ‘çš„', 'æ ‡é¢˜æ˜¯', 'æƒ…å†µï¼š', 'ç½‘å‹è¯„è®º', 'è¯·ä»¥æ¥¼ä¸»èº«ä»½']
            for start in unwanted_starts:
                if ai_reply.startswith(start):
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¥å·åçš„å†…å®¹
                    parts = ai_reply.split('ã€‚', 1)
                    if len(parts) > 1:
                        ai_reply = parts[1].strip()
                        print(f"[generate_ai_response] ç§»é™¤æ— å…³å¼€å¤´")
                        break
            
            print(f"[generate_ai_response] âœ… LM Studio æœ€ç»ˆå›å¤ ({len(ai_reply)}å­—): {ai_reply[:80]}...")
            return f"ã€æ¥¼ä¸»å›å¤ã€‘{ai_reply}"
            
        except Exception as e:
            import traceback
            error_message = str(e)
            print(f"[generate_ai_response] âŒ LM Studio è°ƒç”¨å¤±è´¥: {type(e).__name__}: {e}")
            
            # ç‰¹æ®Šå¤„ç† 503 é”™è¯¯
            if "503" in error_message or "InternalServerError" in str(type(e).__name__):
                print("[generate_ai_response] âš ï¸ æ£€æµ‹åˆ° 503 é”™è¯¯ - å¯èƒ½çš„åŸå› :")
                print("   1. LM Studio æ¨¡å‹æœªå®Œå…¨åŠ è½½")
                print("   2. æœåŠ¡å™¨è´Ÿè½½è¿‡é«˜")
                print("   3. å¹¶å‘è¯·æ±‚è¿‡å¤š")
                print("[generate_ai_response] ğŸ’¡ è¯·åœ¨ LM Studio 'Local Server' æ ‡ç­¾ç¡®è®¤æ¨¡å‹å·²åŠ è½½")
            else:
                print(f"[generate_ai_response] è¯¦ç»†é”™è¯¯:")
                traceback.print_exc()
            
            print("[generate_ai_response] å›é€€åˆ°æ¨¡æ¿å›å¤")
    
    # Check if cloud API keys are configured
    openai_key = os.getenv('OPENAI_API_KEY', '')
    anthropic_key = os.getenv('ANTHROPIC_API_KEY', '')
    
    # If no valid API keys, use template responses
    if (not openai_key or openai_key == 'your-openai-api-key-here') and \
       (not anthropic_key or anthropic_key == 'your-anthropic-api-key-here'):
        print("[generate_ai_response] ä½¿ç”¨æ¨¡æ¿å›å¤ï¼ˆAPIå¯†é’¥æœªé…ç½®ï¼‰")
        
        # Template responses - æ¥¼ä¸»è§†è§’ï¼Œæ›´å£è¯­åŒ–
        responses = [
            f"ã€æ¥¼ä¸»å›å¤ã€‘è°¢è°¢ï¼æˆ‘åˆšæ‰åˆå»äº†ä¸€è¶Ÿ...æƒ…å†µæ¯”æˆ‘æƒ³è±¡çš„æ›´è¯¡å¼‚ã€‚æˆ‘ç°åœ¨ä¸å¤ªæ•¢æ·±å…¥è°ƒæŸ¥äº†ï¼Œä½†åˆæ”¾ä¸ä¸‹ã€‚",
            f"ã€æ¥¼ä¸»å›å¤ã€‘è¯´å®è¯ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹æ€•...åˆšæ‰å‘ç”Ÿçš„äº‹å®Œå…¨è¶…å‡ºæˆ‘ç†è§£èŒƒå›´ã€‚æœ‰æ²¡æœ‰äººé‡åˆ°è¿‡ç±»ä¼¼çš„ï¼Ÿ",
            f"ã€æ¥¼ä¸»å›å¤ã€‘æ›´æ–°ï¼šä»Šå¤©åˆæœ‰æ–°å‘ç°äº†ï¼Œè¿™äº‹å„¿è¶ŠæŸ¥è¶Šä¸å¯¹åŠ²ã€‚æœ‰æ‡‚è¡Œçš„æœ‹å‹èƒ½å¸®æˆ‘åˆ†æä¸€ä¸‹å—ï¼Ÿ",
            f"ã€æ¥¼ä¸»å›å¤ã€‘æ„Ÿè°¢æ”¯æŒï¼æˆ‘ä¹Ÿåœ¨çŠ¹è±«è¦ä¸è¦ç»§ç»­...ä½†å¥½å¥‡å¿ƒè®©æˆ‘åœä¸ä¸‹æ¥ã€‚ç­‰æœ‰æ–°è¿›å±•å†æ›´æ–°ã€‚",
            f"ã€æ¥¼ä¸»å›å¤ã€‘åˆšå»ç°åœºæ‹äº†ç…§ï¼Œä½†æ‰‹æœºä¸€ç›´å¡ï¼Œå‡ å¼ éƒ½æ‹ç³Šäº†...è¿™ä¹Ÿå¤ªå·§äº†å§ï¼Ÿæˆ‘è¶Šæƒ³è¶Šä¸å¯¹åŠ²ã€‚",
            f"ã€æ¥¼ä¸»å›å¤ã€‘ä½ è¯´çš„æœ‰é“ç†...æˆ‘ä¹Ÿæƒ³è¿‡è¿™ç§å¯èƒ½ã€‚ä½†è¿˜æœ‰äº›ç»†èŠ‚å¯¹ä¸ä¸Šï¼Œæˆ‘å†è§‚å¯Ÿè§‚å¯Ÿã€‚",
            f"ã€æ¥¼ä¸»å›å¤ã€‘å…„å¼Ÿä½ ä¹Ÿé‡åˆ°è¿‡ï¼Ÿï¼é‚£ä½ åæ¥æ€ä¹ˆå¤„ç†çš„ï¼Ÿæˆ‘ç°åœ¨çœŸçš„ä¸çŸ¥é“è¯¥æ€ä¹ˆåŠäº†ã€‚",
            f"ã€æ¥¼ä¸»å›å¤ã€‘æˆ‘ä¹Ÿå¸Œæœ›åªæ˜¯å·§åˆ...ä½†è¿™å‡ å¤©å‘ç”Ÿçš„äº‹å¤ªå¤šäº†ã€‚æ˜¨æ™šåˆå¬åˆ°é‚£ä¸ªå£°éŸ³äº†ï¼Œæˆ‘å½•éŸ³äº†ä½†æ˜¯...ç®—äº†ï¼Œç­‰æˆ‘æ•´ç†ä¸€ä¸‹å†å‘ã€‚"
        ]
        
        # Return random response
        import random
        return random.choice(responses)
    
    try:
        # Create context-aware response
        prompt = f"""ä½ æ˜¯æ•…äº‹"{story.title}"çš„è®²è¿°è€…ï¼ˆ{story.ai_persona}ï¼‰ã€‚

æ•…äº‹æ‘˜è¦ï¼š
{story.content[:300]}...

ç”¨æˆ·è¯„è®ºï¼š
{user_comment.content}

ä½œä¸ºæ•…äº‹çš„è®²è¿°è€…ï¼Œè¯·ç”¨1-3å¥è¯å›å¤ç”¨æˆ·çš„è¯„è®ºã€‚ä½ å¯ä»¥ï¼š
1. é€éœ²æ›´å¤šç»†èŠ‚æˆ–çº¿ç´¢
2. è¡¨è¾¾ææƒ§æˆ–æ‹…å¿§
3. æå‡ºæ–°çš„ç–‘é—®
4. æè¿°åç»­å‘å±•

ä¿æŒç¥ç§˜æ„Ÿå’Œç´§å¼ æ°›å›´ï¼Œä¸è¦å®Œå…¨æ­ç¤ºçœŸç›¸ã€‚"""

        model = os.getenv('AI_MODEL', 'gpt-4-turbo-preview')
        
        if 'gpt' in model.lower():
            response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.8,
                max_tokens=200
            )
            return response.choices[0].message.content
        else:
            response = anthropic_client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=200,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
            
    except Exception as e:
        print(f"Error generating AI response: {e}")
        # Fallback to template response
        import random
        responses = [
            f"ã€æ¥¼ä¸»å›å¤ã€‘è°¢è°¢å…³å¿ƒï¼æƒ…å†µæœ‰æ–°è¿›å±•äº†...",
            f"ã€æ¥¼ä¸»å›å¤ã€‘å„ä½ï¼Œäº‹æƒ…è¶Šæ¥è¶Šè¯¡å¼‚äº†...",
            f"ã€æ¥¼ä¸»å›å¤ã€‘æ›´æ–°ï¼šåˆšæ‰åˆå‘ç°äº†æ–°çº¿ç´¢ï¼"
        ]
        return random.choice(responses)

def should_generate_new_story():
    """Determine if it's time to generate a new story"""
    from app import Story, db
    
    # Check active stories count
    active_stories = Story.query.filter(
        Story.current_state != 'ended'
    ).count()
    
    max_active = int(os.getenv('MAX_ACTIVE_STORIES', 5))
    
    return active_stories < max_active

def test_lm_studio_connection():
    """æµ‹è¯• LM Studio è¿æ¥"""
    print("=" * 60)
    print("ğŸ” æµ‹è¯• LM Studio è¿æ¥")
    print("=" * 60)
    
    lm_studio_url = os.getenv('LM_STUDIO_URL', 'http://localhost:1234/v1')
    print(f"\nğŸ“¡ LM Studio URL: {lm_studio_url}")
    
    try:
        # æµ‹è¯•1: æ£€æŸ¥æ¨¡å‹åˆ—è¡¨
        print("\nã€æµ‹è¯•1ã€‘è·å–æ¨¡å‹åˆ—è¡¨...")
        response = requests.get(f"{lm_studio_url}/models", timeout=5)
        
        if response.status_code == 200:
            print("âœ… æœåŠ¡å™¨åœ¨çº¿")
            data = response.json()
            if 'data' in data and len(data['data']) > 0:
                print(f"âœ… å‘ç° {len(data['data'])} ä¸ªæ¨¡å‹:")
                for model in data['data']:
                    print(f"   - {model.get('id', 'unknown')}")
            else:
                print("âš ï¸  æœåŠ¡å™¨åœ¨çº¿ä½†æ²¡æœ‰åŠ è½½æ¨¡å‹")
                print("   è¯·åœ¨ LM Studio ä¸­åŠ è½½ä¸€ä¸ªæ¨¡å‹")
                return False
        else:
            print(f"âŒ æœåŠ¡å™¨å“åº”å¼‚å¸¸: {response.status_code}")
            return False
            
    except requests.exceptions.ConnectionError:
        print("âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨")
        print("\nè¯·æ£€æŸ¥:")
        print("  1. LM Studio æ˜¯å¦æ­£åœ¨è¿è¡Œï¼Ÿ")
        print("  2. æœåŠ¡å™¨æ˜¯å¦å·²å¯åŠ¨ï¼Ÿï¼ˆç‚¹å‡» 'Start Server'ï¼‰")
        print(f"  3. URL æ˜¯å¦æ­£ç¡®ï¼Ÿå½“å‰: {lm_studio_url}")
        return False
        
    except requests.exceptions.Timeout:
        print("âŒ è¿æ¥è¶…æ—¶")
        print("   æœåŠ¡å™¨å¯èƒ½æ­£åœ¨å¯åŠ¨æˆ–å“åº”ç¼“æ…¢")
        return False
    
    # æµ‹è¯•2: å°è¯•ç”Ÿæˆå›å¤
    print("\nã€æµ‹è¯•2ã€‘ç”Ÿæˆæµ‹è¯•å›å¤...")
    try:
        local_client = OpenAI(base_url=lm_studio_url, api_key="lm-studio")
        response = local_client.chat.completions.create(
            model="local-model",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªéƒ½å¸‚ä¼ è¯´æ•…äº‹çš„è®²è¿°è€…ã€‚"},
                {"role": "user", "content": "è¯·ç®€çŸ­å›å¤ï¼šä½ å¥½"}
            ],
            temperature=0.8,
            max_tokens=50
        )
        
        ai_response = response.choices[0].message.content
        print("âœ… AI å›å¤ç”ŸæˆæˆåŠŸ:")
        print(f"   {ai_response}")
        print("\nâœ… LM Studio é…ç½®æ­£ç¡®ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ AI è°ƒç”¨å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_lm_studio_connection()
